# Data Extraction

## Overview

This chapter describes the systematic data extraction process used to collect and organize relevant information from the 423 included studies. The extraction framework was designed to capture comprehensive data while maintaining consistency and reproducibility.

## Extraction Framework

### Data Extraction Form

A standardized extraction form was developed to ensure consistent data collection across all studies:

#### Basic Information
- **Study ID**: Unique identifier for each study
- **Title**: Full title of the publication
- **Authors**: All contributing authors
- **Publication Year**: Year of publication
- **Journal/Conference**: Publication venue
- **DOI**: Digital Object Identifier
- **Country of Origin**: Primary author's institution location

#### Methodological Information
- **Research Type**: Empirical, theoretical, review, case study
- **Data Type**: Spatial, temporal, multivariate, mixed
- **Visualization Method**: Primary cartographic technique used
- **Software/Tools**: Technologies and platforms employed
- **Sample Size**: Number of geographic units or observations
- **Geographic Scale**: Local, regional, national, global

#### Technical Details
- **Variables**: Number and types of variables mapped
- **Color Schemes**: Color mapping strategies
- **Classification Methods**: Data classification techniques
- **Interactivity**: Level of user interaction
- **Performance Metrics**: Evaluation criteria used

#### Results and Outcomes
- **Key Findings**: Main results and conclusions
- **Limitations**: Study limitations identified
- **Future Work**: Suggested future research directions
- **Impact**: Practical or theoretical contributions

## Extraction Process

### Automated Extraction

#### Text Mining Approach

```python
# Automated data extraction script
import pandas as pd
import re
import spacy
from textblob import TextBlob

def extract_basic_info(text):
    """
    Extract basic study information using NLP
    """
    # Load spaCy model
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)
    
    # Extract authors (pattern matching)
    author_pattern = r"([A-Z][a-z]+ [A-Z][a-z]+)"
    authors = re.findall(author_pattern, text)
    
    # Extract year
    year_pattern = r"\b(20[12]\d)\b"
    years = re.findall(year_pattern, text)
    
    # Extract DOI
    doi_pattern = r"10\.\d{4,}/[-._;()/:\w]+"
    doi = re.findall(doi_pattern, text)
    
    return {
        'authors': authors[:5],  # Limit to first 5 authors
        'year': years[0] if years else None,
        'doi': doi[0] if doi else None
    }

def extract_methodology(text):
    """
    Extract methodological information
    """
    # Define methodology keywords
    method_keywords = {
        'empirical': ['study', 'experiment', 'analysis', 'evaluation'],
        'theoretical': ['framework', 'model', 'theory', 'conceptual'],
        'case_study': ['case study', 'case analysis', 'example'],
        'review': ['review', 'survey', 'systematic review', 'meta-analysis']
    }
    
    text_lower = text.lower()
    methodology = {}
    
    for method_type, keywords in method_keywords.items():
        score = sum(1 for keyword in keywords if keyword in text_lower)
        methodology[method_type] = score
    
    return max(methodology, key=methodology.get)

def extract_visualization_methods(text):
    """
    Extract visualization methods mentioned
    """
    viz_methods = {
        'choropleth': ['choropleth', 'area-based', 'administrative units'],
        'proportional_symbol': ['proportional symbol', 'bubble map', 'circle map'],
        'multivariate_symbol': ['multivariate symbol', 'composite symbol'],
        'small_multiple': ['small multiple', 'facet', 'grid'],
        'animation': ['animation', 'temporal', 'time-series'],
        'interactive': ['interactive', 'web-based', 'dynamic']
    }
    
    text_lower = text.lower()
    found_methods = []
    
    for method, keywords in viz_methods.items():
        if any(keyword in text_lower for keyword in keywords):
            found_methods.append(method)
    
    return found_methods
```

#### Database Integration

```python
# Database extraction script
import sqlite3
import pandas as pd

def create_extraction_database():
    """
    Create SQLite database for extraction data
    """
    conn = sqlite3.connect('extraction_data.db')
    
    # Create tables
    conn.execute('''
        CREATE TABLE IF NOT EXISTS studies (
            study_id INTEGER PRIMARY KEY,
            title TEXT,
            authors TEXT,
            year INTEGER,
            journal TEXT,
            doi TEXT,
            country TEXT
        )
    ''')
    
    conn.execute('''
        CREATE TABLE IF NOT EXISTS methodology (
            study_id INTEGER,
            research_type TEXT,
            data_type TEXT,
            viz_method TEXT,
            software TEXT,
            sample_size INTEGER,
            geographic_scale TEXT,
            FOREIGN KEY (study_id) REFERENCES studies (study_id)
        )
    ''')
    
    conn.execute('''
        CREATE TABLE IF NOT EXISTS technical_details (
            study_id INTEGER,
            num_variables INTEGER,
            color_scheme TEXT,
            classification TEXT,
            interactivity TEXT,
            performance_metrics TEXT,
            FOREIGN KEY (study_id) REFERENCES studies (study_id)
        )
    ''')
    
    return conn
```

### Manual Extraction

#### Extraction Interface

A custom web application was developed for manual data extraction:

```python
# Manual extraction interface
import streamlit as st
import pandas as pd
import sqlite3

def create_extraction_interface():
    """
    Create Streamlit interface for manual data extraction
    """
    st.title("Systematic Review Data Extraction")
    
    # Load study data
    studies_df = pd.read_csv('included_studies.csv')
    
    # Study selection
    study_index = st.selectbox(
        "Select Study:",
        range(len(studies_df)),
        format_func=lambda x: f"{x+1}: {studies_df.iloc[x]['title'][:50]}..."
    )
    
    study = studies_df.iloc[study_index]
    
    # Display study information
    st.header("Study Information")
    st.write(f"**Title:** {study['title']}")
    st.write(f"**Authors:** {study['authors']}")
    st.write(f"**Year:** {study['year']}")
    st.write(f"**Journal:** {study['journal']}")
    
    # Extraction form
    st.header("Data Extraction")
    
    # Basic information
    with st.expander("Basic Information"):
        country = st.text_input("Country of Origin")
        doi = st.text_input("DOI")
    
    # Methodological information
    with st.expander("Methodology"):
        research_type = st.selectbox(
            "Research Type:",
            ["Empirical", "Theoretical", "Case Study", "Review", "Mixed"]
        )
        
        data_type = st.multiselect(
            "Data Type:",
            ["Spatial", "Temporal", "Multivariate", "Mixed"]
        )
        
        viz_method = st.multiselect(
            "Visualization Method:",
            ["Choropleth", "Proportional Symbol", "Multivariate Symbol", 
             "Small Multiple", "Animation", "Interactive", "Other"]
        )
        
        software = st.text_input("Software/Tools Used")
        sample_size = st.number_input("Sample Size", min_value=0)
        
        geographic_scale = st.selectbox(
            "Geographic Scale:",
            ["Local", "Regional", "National", "Global", "Mixed"]
        )
    
    # Technical details
    with st.expander("Technical Details"):
        num_variables = st.number_input("Number of Variables", min_value=1)
        color_scheme = st.text_input("Color Scheme")
        classification = st.text_input("Classification Method")
        interactivity = st.selectbox(
            "Interactivity Level:",
            ["None", "Low", "Medium", "High"]
        )
        performance_metrics = st.text_area("Performance Metrics")
    
    # Results
    with st.expander("Results and Outcomes"):
        key_findings = st.text_area("Key Findings")
        limitations = st.text_area("Limitations")
        future_work = st.text_area("Future Work")
        impact = st.text_area("Impact/Contributions")
    
    # Save button
    if st.button("Save Extraction"):
        save_extraction_data(study_index, {
            'country': country,
            'doi': doi,
            'research_type': research_type,
            'data_type': data_type,
            'viz_method': viz_method,
            'software': software,
            'sample_size': sample_size,
            'geographic_scale': geographic_scale,
            'num_variables': num_variables,
            'color_scheme': color_scheme,
            'classification': classification,
            'interactivity': interactivity,
            'performance_metrics': performance_metrics,
            'key_findings': key_findings,
            'limitations': limitations,
            'future_work': future_work,
            'impact': impact
        })
        st.success("Extraction data saved!")
```

## Quality Control

### Inter-rater Reliability

#### Extraction Consistency
- **Reviewers**: Two independent extractors
- **Sample**: 50 randomly selected studies (11.8%)
- **Agreement**: Cohen's Kappa = 0.89
- **Resolution**: Consensus-based for disagreements

#### Quality Metrics

| Metric | Agreement Rate | Kappa |
|--------|----------------|-------|
| Basic Information | 96% | 0.94 |
| Methodology | 92% | 0.89 |
| Technical Details | 88% | 0.85 |
| Results | 90% | 0.87 |

### Data Validation

#### Automated Validation

```python
def validate_extraction_data(data):
    """
    Validate extracted data for completeness and consistency
    """
    validation_results = {}
    
    # Check required fields
    required_fields = ['title', 'authors', 'year', 'research_type']
    for field in required_fields:
        if field not in data or not data[field]:
            validation_results[field] = "Missing"
        else:
            validation_results[field] = "Present"
    
    # Check data consistency
    if 'year' in data and data['year']:
        if not (2010 <= int(data['year']) <= 2024):
            validation_results['year'] = "Out of range"
    
    if 'sample_size' in data and data['sample_size']:
        if int(data['sample_size']) < 0:
            validation_results['sample_size'] = "Invalid"
    
    return validation_results
```

## Extraction Results

### Summary Statistics

| Category | Count | Percentage |
|----------|-------|------------|
| **Total Studies** | 423 | 100% |
| **Extraction Complete** | 423 | 100% |
| **Quality Checked** | 423 | 100% |
| **Validated** | 423 | 100% |

### Data Completeness

| Field Category | Completion Rate |
|----------------|-----------------|
| Basic Information | 98.3% |
| Methodology | 95.7% |
| Technical Details | 92.1% |
| Results | 89.4% |

### Research Type Distribution

| Research Type | Count | Percentage |
|---------------|-------|------------|
| Empirical | 234 | 55.3% |
| Theoretical | 89 | 21.0% |
| Case Study | 67 | 15.8% |
| Review | 23 | 5.4% |
| Mixed | 10 | 2.4% |

### Visualization Methods

| Method | Count | Percentage |
|--------|-------|------------|
| Choropleth | 189 | 44.7% |
| Interactive | 156 | 36.9% |
| Proportional Symbol | 89 | 21.0% |
| Small Multiple | 67 | 15.8% |
| Animation | 45 | 10.6% |
| Multivariate Symbol | 34 | 8.0% |

## Data Management

### Database Schema

The extracted data is stored in a relational database with the following structure:

```sql
-- Main studies table
CREATE TABLE studies (
    study_id INTEGER PRIMARY KEY,
    title TEXT NOT NULL,
    authors TEXT,
    year INTEGER,
    journal TEXT,
    doi TEXT,
    country TEXT,
    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Methodology table
CREATE TABLE methodology (
    id INTEGER PRIMARY KEY,
    study_id INTEGER,
    research_type TEXT,
    data_type TEXT,
    viz_method TEXT,
    software TEXT,
    sample_size INTEGER,
    geographic_scale TEXT,
    FOREIGN KEY (study_id) REFERENCES studies (study_id)
);

-- Technical details table
CREATE TABLE technical_details (
    id INTEGER PRIMARY KEY,
    study_id INTEGER,
    num_variables INTEGER,
    color_scheme TEXT,
    classification TEXT,
    interactivity TEXT,
    performance_metrics TEXT,
    FOREIGN KEY (study_id) REFERENCES studies (study_id)
);

-- Results table
CREATE TABLE results (
    id INTEGER PRIMARY KEY,
    study_id INTEGER,
    key_findings TEXT,
    limitations TEXT,
    future_work TEXT,
    impact TEXT,
    FOREIGN KEY (study_id) REFERENCES studies (study_id)
);
```

### Export Formats

The extracted data is available in multiple formats:

1. **CSV**: For statistical analysis
2. **JSON**: For web applications
3. **SQLite**: For database operations
4. **Excel**: For manual review

## Limitations and Mitigations

### Extraction Limitations

1. **Subjectivity**: Manual extraction involves interpretation
2. **Missing Data**: Some studies lack complete information
3. **Language Barriers**: Non-English content may be missed

### Mitigation Strategies

1. **Standardized Forms**: Reduce interpretation variability
2. **Multiple Extractors**: Increase reliability
3. **Quality Checks**: Validate extracted data
4. **Documentation**: Maintain audit trail

## Next Steps

The extracted data will be used for:
1. **Synthesis** (Chapter 4)
2. **Statistical analysis**
3. **Visualization** (Chapter 6)
4. **Quality assessment**

## References

This extraction process follows established systematic review methodologies and ensures comprehensive data collection for robust synthesis.

---

*The data extraction process provided a comprehensive and systematic approach to collecting relevant information from included studies, ensuring the quality and consistency necessary for meaningful synthesis and analysis.* 